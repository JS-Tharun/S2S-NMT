{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import re\n",
    "import tensorflow as tf\n",
    "import unicodedata\n",
    "import os\n",
    "\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "with open('Datasets\\\\Spa-Eng\\\\Spa_Eng_train_file.txt') as file:\n",
    "  train = [line.rstrip() for line in file]\n",
    "\n",
    "# Separate the input (Spa) and target (Eng) sentences into separate lists.\n",
    "SEPARATOR = '\\t'\n",
    "train_target, train_input = map(list, zip(*[pair.split(SEPARATOR)[:2] for pair in train]))\n",
    "\n",
    "# Unicode normalization\n",
    "def normalize_unicode(s):\n",
    "    return ''.join(c for c in unicodedata.normalize('NFD', s) if unicodedata.category(c) != 'Mn')\n",
    "\n",
    "def preprocess_sentence(s):\n",
    "  s = normalize_unicode(s)\n",
    "  s = re.sub(r\"([?.!,Â¿])\", r\" \\1 \", s)\n",
    "  s = re.sub(r'[\" \"]+', \" \", s)\n",
    "  s = s.strip()\n",
    "  return s\n",
    "\n",
    "# Preprocess both the source and target sentences.\n",
    "train_preprocessed_input = [preprocess_sentence(s) for s in train_input]\n",
    "train_preprocessed_target = [preprocess_sentence(s) for s in train_target]\n",
    "\n",
    "def tag_target_sentences(sentences):\n",
    "  tagged_sentences = map(lambda s: (' ').join(['<sos>', s, '<eos>']), sentences)\n",
    "  return list(tagged_sentences)\n",
    "\n",
    "train_tagged_preprocessed_target = tag_target_sentences(train_preprocessed_target)\n",
    "\n",
    "# Tokenizer for the Hungarian input sentences. Note how we're not filtering punctuation.\n",
    "source_tokenizer = tf.keras.preprocessing.text.Tokenizer(oov_token='<unk>', filters='\"#$%&()*+-/:;=@[\\\\]^_`{|}~\\t\\n')\n",
    "source_tokenizer.fit_on_texts(train_preprocessed_input)\n",
    "source_tokenizer.get_config()\n",
    "\n",
    "source_vocab_size = len(source_tokenizer.word_index) + 1\n",
    "\n",
    "# Tokenizer for the English target sentences.\n",
    "target_tokenizer = tf.keras.preprocessing.text.Tokenizer(oov_token='<unk>', filters='\"#$%&()*+-/:;=@[\\\\]^_`{|}~\\t\\n')\n",
    "target_tokenizer.fit_on_texts(train_tagged_preprocessed_target)\n",
    "target_tokenizer.get_config()\n",
    "\n",
    "target_vocab_size = len(target_tokenizer.word_index) + 1\n",
    "\n",
    "train_encoder_inputs = source_tokenizer.texts_to_sequences(train_preprocessed_input)\n",
    "\n",
    "def generate_decoder_inputs_targets(sentences, tokenizer):\n",
    "  seqs = tokenizer.texts_to_sequences(sentences)\n",
    "  decoder_inputs = [s[:-1] for s in seqs] # Drop the last token in the sentence.\n",
    "  decoder_targets = [s[1:] for s in seqs] # Drop the first token in the sentence.\n",
    "\n",
    "  return decoder_inputs, decoder_targets\n",
    "\n",
    "train_decoder_inputs, train_decoder_targets = generate_decoder_inputs_targets(train_tagged_preprocessed_target, \n",
    "                                                                              target_tokenizer)\n",
    "\n",
    "max_encoding_len = len(max(train_encoder_inputs, key=len))\n",
    "max_decoding_len = len(max(train_decoder_inputs, key=len))\n",
    "\n",
    "padded_train_encoder_inputs = pad_sequences(train_encoder_inputs, max_encoding_len, padding='post', truncating='post')\n",
    "padded_train_decoder_inputs = pad_sequences(train_decoder_inputs, max_decoding_len, padding='post', truncating='post')\n",
    "padded_train_decoder_targets = pad_sequences(train_decoder_targets, max_decoding_len, padding='post', truncating='post')\n",
    "\n",
    "target_tokenizer.sequences_to_texts([padded_train_decoder_inputs[0]])\n",
    "\n",
    "with open('Datasets\\\\Spa-Eng\\\\Spa_Eng_Val_file.txt') as file:\n",
    "  val = [line.rstrip() for line in file]\n",
    "\n",
    "def process_dataset(dataset):\n",
    "\n",
    "  # Split the Hungarian and English sentences into separate lists.\n",
    "  output, input = map(list, zip(*[pair.split(SEPARATOR)[:2] for pair in dataset]))\n",
    "\n",
    "  # Unicode normalization and inserting spaces around punctuation.\n",
    "  preprocessed_input = [preprocess_sentence(s) for s in input]\n",
    "  preprocessed_output = [preprocess_sentence(s) for s in output]\n",
    "\n",
    "  # Tag target sentences with <sos> and <eos> tokens.\n",
    "  tagged_preprocessed_output = tag_target_sentences(preprocessed_output)\n",
    "\n",
    "  # Vectorize encoder source sentences.\n",
    "  encoder_inputs = source_tokenizer.texts_to_sequences(preprocessed_input)\n",
    "\n",
    "  # Vectorize and create decoder input and target sentences.\n",
    "  decoder_inputs, decoder_targets = generate_decoder_inputs_targets(tagged_preprocessed_output, \n",
    "                                                                    target_tokenizer)\n",
    "  \n",
    "  # Pad all collections.\n",
    "  padded_encoder_inputs = pad_sequences(encoder_inputs, max_encoding_len, padding='post', truncating='post')\n",
    "  padded_decoder_inputs = pad_sequences(decoder_inputs, max_decoding_len, padding='post', truncating='post')\n",
    "  padded_decoder_targets = pad_sequences(decoder_targets, max_decoding_len, padding='post', truncating='post')\n",
    "\n",
    "  return padded_encoder_inputs, padded_decoder_inputs, padded_decoder_targets\n",
    "\n",
    "# Process validation dataset\n",
    "padded_val_encoder_inputs, padded_val_decoder_inputs, padded_val_decoder_targets = process_dataset(val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_dim = 128\n",
    "hidden_dim = 256\n",
    "default_dropout=0.2\n",
    "batch_size = 128\n",
    "epochs = 30\n",
    "\n",
    "# Fixed input shape: (batch_size=1, sequence_length=79 for encoder, 81 for decoder)\n",
    "encoder_inputs = layers.Input(shape=(78,), name='encoder_inputs')\n",
    "\n",
    "# Fixed decoder input shape: (batch_size=1, sequence_length=81)\n",
    "decoder_inputs = layers.Input(shape=(81,), name='decoder_inputs')\n",
    "\n",
    "# Keep the rest of the model structure the same\n",
    "encoder_embeddings = layers.Embedding(source_vocab_size, \n",
    "                                       embedding_dim, \n",
    "                                       mask_zero=True, \n",
    "                                       name='encoder_embeddings')(encoder_inputs)\n",
    "\n",
    "encoder_outputs, encoder_state = layers.GRU(hidden_dim, \n",
    "                                            return_sequences=True, \n",
    "                                            return_state=True, \n",
    "                                            dropout=default_dropout, \n",
    "                                            name='encoder_gru')(encoder_embeddings)\n",
    "\n",
    "decoder_embeddings = layers.Embedding(target_vocab_size, \n",
    "                                       embedding_dim, \n",
    "                                       mask_zero=True, \n",
    "                                       name='decoder_embeddings')(decoder_inputs)\n",
    "\n",
    "decoder_outputs, decoder_state = layers.GRU(hidden_dim, \n",
    "                                            return_sequences=True, \n",
    "                                            return_state=True, \n",
    "                                            dropout=default_dropout, \n",
    "                                            name='decoder_gru')(decoder_embeddings, \n",
    "                                                                initial_state=encoder_state)\n",
    "\n",
    "attention_scores = layers.Dot(axes=[2, 2], name='attention_scores')([decoder_outputs, encoder_outputs])\n",
    "attention_weights = layers.Activation('softmax', name='attention_weights')(attention_scores)\n",
    "context_vector = layers.Dot(axes=[2, 1], name='context_vector')([attention_weights, encoder_outputs])\n",
    "decoder_combined_context = layers.Concatenate(axis=-1, name='decoder_combined_context')([context_vector, decoder_outputs])\n",
    "\n",
    "y_proba = layers.Dense(target_vocab_size, activation='softmax', name='decoder_dense')(decoder_combined_context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"fixed_input_gru_luong_attention\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " encoder_inputs (InputLayer)    [(None, 78)]         0           []                               \n",
      "                                                                                                  \n",
      " decoder_inputs (InputLayer)    [(None, 81)]         0           []                               \n",
      "                                                                                                  \n",
      " encoder_embeddings (Embedding)  (None, 78, 128)     3210624     ['encoder_inputs[0][0]']         \n",
      "                                                                                                  \n",
      " decoder_embeddings (Embedding)  (None, 81, 128)     1711744     ['decoder_inputs[0][0]']         \n",
      "                                                                                                  \n",
      " encoder_gru (GRU)              [(None, 78, 256),    296448      ['encoder_embeddings[0][0]']     \n",
      "                                 (None, 256)]                                                     \n",
      "                                                                                                  \n",
      " decoder_gru (GRU)              [(None, 81, 256),    296448      ['decoder_embeddings[0][0]',     \n",
      "                                 (None, 256)]                     'encoder_gru[0][1]']            \n",
      "                                                                                                  \n",
      " attention_scores (Dot)         (None, 81, 78)       0           ['decoder_gru[0][0]',            \n",
      "                                                                  'encoder_gru[0][0]']            \n",
      "                                                                                                  \n",
      " attention_weights (Activation)  (None, 81, 78)      0           ['attention_scores[0][0]']       \n",
      "                                                                                                  \n",
      " context_vector (Dot)           (None, 81, 256)      0           ['attention_weights[0][0]',      \n",
      "                                                                  'encoder_gru[0][0]']            \n",
      "                                                                                                  \n",
      " decoder_combined_context (Conc  (None, 81, 512)     0           ['context_vector[0][0]',         \n",
      " atenate)                                                         'decoder_gru[0][0]']            \n",
      "                                                                                                  \n",
      " decoder_dense (Dense)          (None, 81, 13373)    6860349     ['decoder_combined_context[0][0]'\n",
      "                                                                 ]                                \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 12,375,613\n",
      "Trainable params: 12,375,613\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Define the model with encoder and decoder inputs\n",
    "model_with_attention = tf.keras.Model([encoder_inputs, decoder_inputs], y_proba, name='fixed_input_gru_luong_attention')\n",
    "\n",
    "# Compile the model\n",
    "model_with_attention.compile(optimizer='adam', \n",
    "                             loss='sparse_categorical_crossentropy', \n",
    "                             metrics=['sparse_categorical_accuracy'])\n",
    "\n",
    "# Display model summary\n",
    "model_with_attention.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "885/885 [==============================] - ETA: 0s - loss: 0.0541 - sparse_categorical_accuracy: 0.8604\n",
      "Epoch 1: saving model to Weights\\FixedInputGRULuongAttention.weights.h5\n",
      "885/885 [==============================] - 144s 152ms/step - loss: 0.0541 - sparse_categorical_accuracy: 0.8604 - val_loss: 0.1558 - val_sparse_categorical_accuracy: 0.7285\n"
     ]
    }
   ],
   "source": [
    "# Define callbacks\n",
    "checkpoint_filepath = 'Weights/FixedInputGRULuongAttention.weights.h5'\n",
    "checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(filepath=checkpoint_filepath,\n",
    "                                                         save_weights_only=True,\n",
    "                                                         verbose=1)\n",
    "\n",
    "early_stopping_callback = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=3)\n",
    "\n",
    "model_with_attention.load_weights(\"Weights/FixedInputGRULuongAttention.weights.h5\")\n",
    "\n",
    "# Train the model\n",
    "history = model_with_attention.fit(\n",
    "    [padded_train_encoder_inputs, padded_train_decoder_inputs], \n",
    "    padded_train_decoder_targets,\n",
    "    batch_size=batch_size,\n",
    "    epochs=1,\n",
    "    validation_data=([padded_val_encoder_inputs, padded_val_decoder_inputs], padded_val_decoder_targets),\n",
    "    callbacks=[checkpoint_callback, early_stopping_callback]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_with_attention.save('SavedModel/FixedInputGRULuongAttention.keras')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "source_tokenizer_json = source_tokenizer.to_json()\n",
    "with io.open('WordTokenizers/fixed_source_tokenizer.json', 'w', encoding='utf-8') as f:\n",
    "    f.write(json.dumps(source_tokenizer_json, ensure_ascii=False))\n",
    "\n",
    "target_tokenizer_json = target_tokenizer.to_json()\n",
    "with io.open('WordTokenizers/fixed_target_tokenizer.json', 'w', encoding='utf-8') as f:\n",
    "    f.write(json.dumps(target_tokenizer_json, ensure_ascii=False))\n",
    "\n",
    "with open('WordTokenizers\\\\fixed_source_tokenizer.json') as f:\n",
    "    data = json.load(f)\n",
    "    source_tokenizer = tf.keras.preprocessing.text.tokenizer_from_json(data)\n",
    "\n",
    "with open('WordTokenizers\\\\fixed_target_tokenizer.json') as f:\n",
    "    data = json.load(f)\n",
    "    target_tokenizer = tf.keras.preprocessing.text.tokenizer_from_json(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "443/443 [==============================] - 10s 22ms/step - loss: 0.1548 - sparse_categorical_accuracy: 0.7289\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['encoder_inputs',\n",
       " 'decoder_inputs',\n",
       " 'encoder_embeddings',\n",
       " 'decoder_embeddings',\n",
       " 'encoder_gru',\n",
       " 'decoder_gru',\n",
       " 'attention_scores',\n",
       " 'attention_weights',\n",
       " 'context_vector',\n",
       " 'decoder_combined_context',\n",
       " 'decoder_dense']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open('Datasets\\\\Spa-Eng\\\\Spa_Eng_test_file.txt') as file:\n",
    "  test = [line.rstrip() for line in file]\n",
    "\n",
    "# Preprocess test dataset\n",
    "padded_test_encoder_inputs, padded_test_decoder_inputs, padded_test_decoder_targets = process_dataset(test)\n",
    "\n",
    "# Evaluate the model on the test set.\n",
    "model_with_attention.evaluate([padded_test_encoder_inputs, padded_test_decoder_inputs], padded_test_decoder_targets)\n",
    "\n",
    "# These are the layers of our trained model.\n",
    "[layer.name for layer in model_with_attention.layers]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spanish: Lo mejor de las suertes\n",
      "English: do you prefer the best .\n"
     ]
    }
   ],
   "source": [
    "def translate_with_attention(input_sentence, model, source_tokenizer, target_tokenizer, max_encoding_len, max_decoding_len):\n",
    "    \"\"\"\n",
    "    Translate a Spanish sentence to English using a trained Seq2Seq model with Luong Attention.\n",
    "\n",
    "    Parameters:\n",
    "    - input_sentence: str, input sentence in Spanish.\n",
    "    - model: tf.keras.Model, the trained model with attention.\n",
    "    - source_tokenizer: Tokenizer for the source language (Spanish).\n",
    "    - target_tokenizer: Tokenizer for the target language (English).\n",
    "    - max_encoding_len: int, maximum encoding sequence length.\n",
    "    - max_decoding_len: int, maximum decoding sequence length.\n",
    "\n",
    "    Returns:\n",
    "    - Translated sentence as a string.\n",
    "    \"\"\"\n",
    "    # Preprocess input sentence\n",
    "    preprocessed_sentence = preprocess_sentence(input_sentence)\n",
    "\n",
    "    # Tokenize and pad the input sentence\n",
    "    input_sequence = source_tokenizer.texts_to_sequences([preprocessed_sentence])\n",
    "    padded_input_sequence = pad_sequences(input_sequence, maxlen=max_encoding_len, padding='post')\n",
    "\n",
    "    # Pass the input through the encoder to get the encoder outputs and initial decoder state\n",
    "    encoder_outputs, encoder_state = model.get_layer('encoder_gru')(\n",
    "        model.get_layer('encoder_embeddings')(padded_input_sequence)\n",
    "    )\n",
    "\n",
    "    # Initialize the decoder input with the <sos> token\n",
    "    start_token_index = target_tokenizer.word_index['<sos>']\n",
    "    decoder_input = np.array([[start_token_index]])\n",
    "\n",
    "    # Initialize the decoder state\n",
    "    decoder_state = encoder_state\n",
    "\n",
    "    # Translation result\n",
    "    translated_sentence = []\n",
    "\n",
    "    # Iteratively decode each timestep\n",
    "    for _ in range(max_decoding_len):\n",
    "        # Get decoder embedding for the current input\n",
    "        decoder_embedding_output = model.get_layer('decoder_embeddings')(decoder_input)\n",
    "\n",
    "        # Pass through the decoder GRU\n",
    "        decoder_output, decoder_state = model.get_layer('decoder_gru')(decoder_embedding_output, initial_state=decoder_state)\n",
    "\n",
    "        # Compute attention scores\n",
    "        attention_scores = tf.matmul(decoder_output, encoder_outputs, transpose_b=True)  # Dot product for Luong Attention\n",
    "\n",
    "        # Compute attention weights (softmax over scores)\n",
    "        attention_weights = tf.nn.softmax(attention_scores, axis=-1)\n",
    "\n",
    "        # Compute the context vector as the weighted sum of encoder outputs\n",
    "        context_vector = tf.matmul(attention_weights, encoder_outputs)\n",
    "\n",
    "        # Concatenate context vector and decoder output\n",
    "        decoder_combined_context = tf.concat([context_vector, decoder_output], axis=-1)\n",
    "\n",
    "        # Pass through the dense layer to generate token probabilities\n",
    "        token_probs = model.get_layer('decoder_dense')(decoder_combined_context)\n",
    "\n",
    "        # Get the token with the highest probability\n",
    "        predicted_token_index = tf.argmax(token_probs[0, 0]).numpy()\n",
    "\n",
    "        # Stop if <eos> token is predicted\n",
    "        if predicted_token_index == target_tokenizer.word_index['<eos>']:\n",
    "            break\n",
    "\n",
    "        # Append the predicted word to the translation\n",
    "        translated_sentence.append(target_tokenizer.index_word.get(predicted_token_index, '<unk>'))\n",
    "\n",
    "        # Update the decoder input with the predicted token\n",
    "        decoder_input = np.array([[predicted_token_index]])\n",
    "\n",
    "    return ' '.join(translated_sentence)\n",
    "\n",
    "# Test the inference pipeline with a Spanish sentence\n",
    "spanish_sentence = \"Lo mejor de las suertes\"\n",
    "translated_english_sentence = translate_with_attention(\n",
    "    spanish_sentence, \n",
    "    model_with_attention, \n",
    "    source_tokenizer, \n",
    "    target_tokenizer, \n",
    "    max_encoding_len, \n",
    "    max_decoding_len\n",
    ")\n",
    "\n",
    "print(f\"Spanish: {spanish_sentence}\")\n",
    "print(f\"English: {translated_english_sentence}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as gru_cell_2_layer_call_fn, gru_cell_2_layer_call_and_return_conditional_losses, gru_cell_3_layer_call_fn, gru_cell_3_layer_call_and_return_conditional_losses while saving (showing 4 of 4). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: C:\\Users\\jstha\\AppData\\Local\\Temp\\tmpqgp7aalh\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: C:\\Users\\jstha\\AppData\\Local\\Temp\\tmpqgp7aalh\\assets\n"
     ]
    }
   ],
   "source": [
    "# Convert the model to TFLite format\n",
    "converter = tf.lite.TFLiteConverter.from_keras_model(model_with_attention)\n",
    "\n",
    "# Enable support for Select TensorFlow ops\n",
    "converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS, tf.lite.OpsSet.SELECT_TF_OPS]\n",
    "\n",
    "# Disable experimental lowering of tensor list operations\n",
    "converter._experimental_lower_tensor_list_ops = False\n",
    "\n",
    "# Enable optimization for size and latency\n",
    "converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "\n",
    "# Convert the model\n",
    "tflite_model = converter.convert()\n",
    "\n",
    "# Save the TFLite model to disk\n",
    "with open('TFLiteModel\\\\fixed_input_gru_luong_attention.tflite', 'wb') as f:\n",
    "    f.write(tflite_model)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "GPU",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
