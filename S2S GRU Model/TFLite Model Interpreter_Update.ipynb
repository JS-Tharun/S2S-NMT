{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import unicodedata\n",
    "import re\n",
    "import numpy as np\n",
    "import json\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.text import tokenizer_from_json\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "import numpy as np\n",
    "\n",
    "max_encoding_len = 78\n",
    "max_decoding_len = 81\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the TFLite model\n",
    "interpreter = tf.lite.Interpreter(model_path='TFLiteModel/fixed_input_gru_luong_attention.tflite')\n",
    "# Allocate tensors\n",
    "interpreter.allocate_tensors()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input Details: [{'name': 'serving_default_encoder_inputs:0', 'index': 0, 'shape': array([ 1, 78]), 'shape_signature': array([-1, 78]), 'dtype': <class 'numpy.float32'>, 'quantization': (0.0, 0), 'quantization_parameters': {'scales': array([], dtype=float32), 'zero_points': array([], dtype=int32), 'quantized_dimension': 0}, 'sparsity_parameters': {}}, {'name': 'serving_default_decoder_inputs:0', 'index': 1, 'shape': array([ 1, 81]), 'shape_signature': array([-1, 81]), 'dtype': <class 'numpy.float32'>, 'quantization': (0.0, 0), 'quantization_parameters': {'scales': array([], dtype=float32), 'zero_points': array([], dtype=int32), 'quantized_dimension': 0}, 'sparsity_parameters': {}}]\n",
      "Output Details: [{'name': 'StatefulPartitionedCall:0', 'index': 138, 'shape': array([    1,     1, 13373]), 'shape_signature': array([   -1,    -1, 13373]), 'dtype': <class 'numpy.float32'>, 'quantization': (0.0, 0), 'quantization_parameters': {'scales': array([], dtype=float32), 'zero_points': array([], dtype=int32), 'quantized_dimension': 0}, 'sparsity_parameters': {}}]\n"
     ]
    }
   ],
   "source": [
    "# Get input and output details\n",
    "input_details = interpreter.get_input_details()\n",
    "output_details = interpreter.get_output_details()\n",
    "\n",
    "print(\"Input Details:\", input_details)\n",
    "print(\"Output Details:\", output_details)\n",
    "\n",
    "with open('WordTokenizers\\\\fixed_source_tokenizer.json') as f:\n",
    "    data = json.load(f)\n",
    "    source_tokenizer = tf.keras.preprocessing.text.tokenizer_from_json(data)\n",
    "\n",
    "with open('WordTokenizers\\\\fixed_target_tokenizer.json') as f:\n",
    "    data = json.load(f)\n",
    "    target_tokenizer = tf.keras.preprocessing.text.tokenizer_from_json(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import unicodedata\n",
    "import re\n",
    "import numpy as np\n",
    "import json\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.text import tokenizer_from_json\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "# Load tokenizers\n",
    "def load_tokenizers():\n",
    "    with open('WordTokenizers\\\\fixed_source_tokenizer.json') as f:\n",
    "        data = json.load(f)\n",
    "        source_tokenizer = tf.keras.preprocessing.text.tokenizer_from_json(data)\n",
    "\n",
    "    with open('WordTokenizers\\\\fixed_target_tokenizer.json') as f:\n",
    "        data = json.load(f)\n",
    "        target_tokenizer = tf.keras.preprocessing.text.tokenizer_from_json(data)\n",
    "\n",
    "    return source_tokenizer, target_tokenizer\n",
    "\n",
    "# Define the function to normalize Unicode characters\n",
    "def normalize_unicode(s):\n",
    "    return ''.join(c for c in unicodedata.normalize('NFD', s) if unicodedata.category(c) != 'Mn')\n",
    "\n",
    "# Define the function to preprocess a sentence\n",
    "def preprocess_sentence(s):\n",
    "    s = normalize_unicode(s)\n",
    "    # Add spaces around punctuation marks (., ?, !, etc.)\n",
    "    s = re.sub(r\"([?.!,Â¿])\", r\" \\1 \", s)\n",
    "    # Replace multiple spaces with a single space\n",
    "    s = re.sub(r'[\" \"]+', \" \", s)\n",
    "    # Strip leading and trailing spaces\n",
    "    s = s.strip()\n",
    "    return s\n",
    "\n",
    "# Define the function to tag target sentences\n",
    "def tag_target_sentences(sentences):\n",
    "    tagged_sentences = map(lambda s: (' ').join(['<sos>', s, '<eos>']), sentences)\n",
    "    return list(tagged_sentences)\n",
    "\n",
    "# Combined function to preprocess the input and return padded sequence\n",
    "def preprocess_input_sequence(user_input, max_encoding_len=78):\n",
    "    # Load tokenizers\n",
    "    source_tokenizer, target_tokenizer = load_tokenizers()\n",
    "\n",
    "    # Preprocess the input sentence\n",
    "    unicode_input = normalize_unicode(user_input)\n",
    "    preprocess_input = preprocess_sentence(unicode_input)\n",
    "    \n",
    "    # Tag target sentences (assuming the user input is a target sentence)\n",
    "    tagged_input = tag_target_sentences([preprocess_input])\n",
    "    \n",
    "    # Convert the sentence to a sequence using source tokenizer\n",
    "    train_encoder_inputs = source_tokenizer.texts_to_sequences(tagged_input)\n",
    "    \n",
    "    # Pad the sequence to the maximum encoding length\n",
    "    padded_input_sequence = pad_sequences(train_encoder_inputs, maxlen=max_encoding_len, padding='post')\n",
    "    \n",
    "    return np.array(padded_input_sequence, dtype=np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_sequence_tflite(interpreter, encoder_input, target_tokenizer, max_decoding_len=81):\n",
    "    start_token_index = target_tokenizer.word_index['<sos>']\n",
    "    end_token_index = target_tokenizer.word_index['<eos>']\n",
    "\n",
    "    decoder_input = np.zeros((1, max_decoding_len), dtype=np.float32)\n",
    "    decoder_input[0, 0] = start_token_index  # First token is <sos>\n",
    "\n",
    "    translated_tokens = []\n",
    "\n",
    "    for i in range(max_decoding_len - 1):  # Leave space for <eos>\n",
    "        # Set encoder and decoder inputs\n",
    "        interpreter.set_tensor(input_details[0]['index'], encoder_input)  # Encoder input\n",
    "        interpreter.set_tensor(input_details[1]['index'], decoder_input)  # Decoder input\n",
    "\n",
    "        # Run inference\n",
    "        interpreter.invoke()\n",
    "\n",
    "        # Get the output probabilities\n",
    "        output_probs = interpreter.get_tensor(output_details[0]['index'])  # Verify this index\n",
    "        predicted_token_index = np.argmax(output_probs[0, i])  # Current timestep\n",
    "\n",
    "        # Append the predicted token to result\n",
    "        if predicted_token_index == end_token_index:\n",
    "            break\n",
    "        translated_tokens.append(target_tokenizer.index_word.get(predicted_token_index, '<unk>'))\n",
    "\n",
    "        # Update decoder input for next timestep\n",
    "        if i + 1 < max_decoding_len:\n",
    "            decoder_input[0, i + 1] = predicted_token_index\n",
    "\n",
    "    return ' '.join(translated_tokens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input Details: [{'name': 'serving_default_encoder_inputs:0', 'index': 0, 'shape': array([ 1, 78]), 'shape_signature': array([-1, 78]), 'dtype': <class 'numpy.float32'>, 'quantization': (0.0, 0), 'quantization_parameters': {'scales': array([], dtype=float32), 'zero_points': array([], dtype=int32), 'quantized_dimension': 0}, 'sparsity_parameters': {}}, {'name': 'serving_default_decoder_inputs:0', 'index': 1, 'shape': array([ 1, 81]), 'shape_signature': array([-1, 81]), 'dtype': <class 'numpy.float32'>, 'quantization': (0.0, 0), 'quantization_parameters': {'scales': array([], dtype=float32), 'zero_points': array([], dtype=int32), 'quantized_dimension': 0}, 'sparsity_parameters': {}}]\n",
      "Output Details: [{'name': 'StatefulPartitionedCall:0', 'index': 138, 'shape': array([    1,     1, 13373]), 'shape_signature': array([   -1,    -1, 13373]), 'dtype': <class 'numpy.float32'>, 'quantization': (0.0, 0), 'quantization_parameters': {'scales': array([], dtype=float32), 'zero_points': array([], dtype=int32), 'quantized_dimension': 0}, 'sparsity_parameters': {}}]\n"
     ]
    }
   ],
   "source": [
    "print(\"Input Details:\", interpreter.get_input_details())\n",
    "print(\"Output Details:\", interpreter.get_output_details())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_input_sequence(user_input, max_encoding_len=78):\n",
    "    source_tokenizer, _ = load_tokenizers()\n",
    "\n",
    "    user_input = preprocess_sentence(normalize_unicode(user_input))\n",
    "    tokenized_sequence = source_tokenizer.texts_to_sequences([user_input])\n",
    "    padded_sequence = pad_sequences(tokenized_sequence, maxlen=max_encoding_len, padding='post')\n",
    "\n",
    "    return np.array(padded_sequence, dtype=np.float32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Input Shape: [ 1 78]\n"
     ]
    }
   ],
   "source": [
    "print(\"Model Input Shape:\", input_details[0]['shape'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoder Input Shape: (1, 78)\n",
      "Translated Sentence: go there .\n"
     ]
    }
   ],
   "source": [
    "spanish_sentence = \"Ir allÃ­\"\n",
    "encoder_input = preprocess_input_sequence(spanish_sentence)\n",
    "print(\"Encoder Input Shape:\", encoder_input.shape)\n",
    "\n",
    "translated_sentence = decode_sequence_tflite(interpreter, encoder_input, target_tokenizer)\n",
    "print(\"Translated Sentence:\", translated_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "GPU",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
