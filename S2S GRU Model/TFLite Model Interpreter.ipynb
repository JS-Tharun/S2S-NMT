{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import unicodedata\n",
    "import re\n",
    "import numpy as np\n",
    "import json\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.text import tokenizer_from_json\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "import numpy as np\n",
    "\n",
    "max_encoding_len = 79\n",
    "max_decoding_len = 81\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the TFLite model\n",
    "interpreter = tf.lite.Interpreter(model_path='TFLiteModel/fixed_input_gru_luong_attention.tflite')\n",
    "# Allocate tensors\n",
    "interpreter.allocate_tensors()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input Details: [{'name': 'serving_default_encoder_inputs:0', 'index': 0, 'shape': array([ 1, 78]), 'shape_signature': array([-1, 78]), 'dtype': <class 'numpy.float32'>, 'quantization': (0.0, 0), 'quantization_parameters': {'scales': array([], dtype=float32), 'zero_points': array([], dtype=int32), 'quantized_dimension': 0}, 'sparsity_parameters': {}}, {'name': 'serving_default_decoder_inputs:0', 'index': 1, 'shape': array([ 1, 81]), 'shape_signature': array([-1, 81]), 'dtype': <class 'numpy.float32'>, 'quantization': (0.0, 0), 'quantization_parameters': {'scales': array([], dtype=float32), 'zero_points': array([], dtype=int32), 'quantized_dimension': 0}, 'sparsity_parameters': {}}]\n",
      "Output Details: [{'name': 'StatefulPartitionedCall:0', 'index': 138, 'shape': array([    1,     1, 13373]), 'shape_signature': array([   -1,    -1, 13373]), 'dtype': <class 'numpy.float32'>, 'quantization': (0.0, 0), 'quantization_parameters': {'scales': array([], dtype=float32), 'zero_points': array([], dtype=int32), 'quantized_dimension': 0}, 'sparsity_parameters': {}}]\n"
     ]
    }
   ],
   "source": [
    "# Get input and output details\n",
    "input_details = interpreter.get_input_details()\n",
    "output_details = interpreter.get_output_details()\n",
    "\n",
    "print(\"Input Details:\", input_details)\n",
    "print(\"Output Details:\", output_details)\n",
    "\n",
    "with open('WordTokenizers\\\\fixed_source_tokenizer.json') as f:\n",
    "    data = json.load(f)\n",
    "    source_tokenizer = tf.keras.preprocessing.text.tokenizer_from_json(data)\n",
    "\n",
    "with open('WordTokenizers\\\\fixed_target_tokenizer.json') as f:\n",
    "    data = json.load(f)\n",
    "    target_tokenizer = tf.keras.preprocessing.text.tokenizer_from_json(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import unicodedata\n",
    "import re\n",
    "import numpy as np\n",
    "import json\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.text import tokenizer_from_json\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "# Load tokenizers\n",
    "def load_tokenizers():\n",
    "    with open('WordTokenizers\\\\fixed_source_tokenizer.json') as f:\n",
    "        data = json.load(f)\n",
    "        source_tokenizer = tf.keras.preprocessing.text.tokenizer_from_json(data)\n",
    "\n",
    "    with open('WordTokenizers\\\\fixed_target_tokenizer.json') as f:\n",
    "        data = json.load(f)\n",
    "        target_tokenizer = tf.keras.preprocessing.text.tokenizer_from_json(data)\n",
    "\n",
    "    return source_tokenizer, target_tokenizer\n",
    "\n",
    "# Define the function to normalize Unicode characters\n",
    "def normalize_unicode(s):\n",
    "    return ''.join(c for c in unicodedata.normalize('NFD', s) if unicodedata.category(c) != 'Mn')\n",
    "\n",
    "# Define the function to preprocess a sentence\n",
    "def preprocess_sentence(s):\n",
    "    s = normalize_unicode(s)\n",
    "    # Add spaces around punctuation marks (., ?, !, etc.)\n",
    "    s = re.sub(r\"([?.!,Â¿])\", r\" \\1 \", s)\n",
    "    # Replace multiple spaces with a single space\n",
    "    s = re.sub(r'[\" \"]+', \" \", s)\n",
    "    # Strip leading and trailing spaces\n",
    "    s = s.strip()\n",
    "    return s\n",
    "\n",
    "# Define the function to tag target sentences\n",
    "def tag_target_sentences(sentences):\n",
    "    tagged_sentences = map(lambda s: (' ').join(['<sos>', s, '<eos>']), sentences)\n",
    "    return list(tagged_sentences)\n",
    "\n",
    "# Combined function to preprocess the input and return padded sequence\n",
    "def preprocess_input_sequence(user_input, max_encoding_len=78):\n",
    "    # Load tokenizers\n",
    "    source_tokenizer, target_tokenizer = load_tokenizers()\n",
    "\n",
    "    # Preprocess the input sentence\n",
    "    unicode_input = normalize_unicode(user_input)\n",
    "    preprocess_input = preprocess_sentence(unicode_input)\n",
    "    \n",
    "    # Tag target sentences (assuming the user input is a target sentence)\n",
    "    tagged_input = tag_target_sentences([preprocess_input])\n",
    "    \n",
    "    # Convert the sentence to a sequence using source tokenizer\n",
    "    train_encoder_inputs = source_tokenizer.texts_to_sequences(tagged_input)\n",
    "    \n",
    "    # Pad the sequence to the maximum encoding length\n",
    "    padded_input_sequence = pad_sequences(train_encoder_inputs, maxlen=max_encoding_len, padding='post')\n",
    "    \n",
    "    return np.array(padded_input_sequence, dtype=np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_sequence_tflite(interpreter, encoder_input, target_tokenizer, max_decoding_len=81):\n",
    "\n",
    "    start_token_index = target_tokenizer.word_index['<sos>']\n",
    "    end_token_index = target_tokenizer.word_index['<eos>']\n",
    "\n",
    "    decoder_input = np.zeros((1, 81), dtype=np.float32)\n",
    "    decoder_input[0, 0] = start_token_index  # First token is <sos>\n",
    "\n",
    "    translated_tokens = []\n",
    "\n",
    "    for i in range(max_decoding_len):\n",
    "        # Set encoder and decoder inputs\n",
    "        interpreter.set_tensor(0, encoder_input)  # Encoder input at index 0\n",
    "        interpreter.set_tensor(1, decoder_input)  # Decoder input at index 1\n",
    "\n",
    "        # Run inference\n",
    "        interpreter.invoke()\n",
    "\n",
    "        # Get the output probabilities\n",
    "        output_probs = interpreter.get_tensor(138)  # Output at index 138\n",
    "        predicted_token_index = np.argmax(output_probs[0, 0])  # Pick token with max probability\n",
    "\n",
    "        # Check for end of sentence\n",
    "        if predicted_token_index == end_token_index:\n",
    "            break\n",
    "\n",
    "        # Append token to result\n",
    "        translated_tokens.append(target_tokenizer.index_word.get(predicted_token_index, '<unk>'))\n",
    "\n",
    "        # Update decoder input\n",
    "        if i + 1 < 81:\n",
    "            decoder_input[0, i + 1] = predicted_token_index\n",
    "\n",
    "    return ' '.join(translated_tokens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_input(input_sequence):\n",
    "    # Set the encoder input tensor\n",
    "    interpreter.set_tensor(input_details[0]['index'], input_sequence)\n",
    "    interpreter.invoke()\n",
    "\n",
    "    # Extract the encoder outputs and state\n",
    "    encoder_outputs = interpreter.get_tensor(output_details[0]['index'])\n",
    "    encoder_state = interpreter.get_tensor(output_details[1]['index'])\n",
    "    return encoder_outputs, encoder_state\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate_tflite(input_sentence, interpreter, source_tokenizer, target_tokenizer, max_encoding_len=79, max_decoding_len=81):\n",
    "    encoder_input = preprocess_input_sequence(input_sentence)\n",
    "    translated_sentence = decode_sequence_tflite(interpreter, encoder_input, target_tokenizer, max_decoding_len)\n",
    "    return translated_sentence\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spanish: Right\n",
      "English: let let let let let let let let let let let let let let let let let let let let let let let let let let let let let let let let let let let let let let let let let let let let let let let let let let let let let let let let let let let let let let let let let let let let let let let let let let let let let let let let let\n"
     ]
    }
   ],
   "source": [
    "# Example Usage:\n",
    "spanish_sentence = \"Right\"\n",
    "# Translate the sentence\n",
    "translated_english_sentence = translate_tflite(\n",
    "    spanish_sentence, \n",
    "    interpreter, \n",
    "    source_tokenizer, \n",
    "    target_tokenizer,\n",
    ")\n",
    "\n",
    "print(f\"Spanish: {spanish_sentence}\")\n",
    "print(f\"English: {translated_english_sentence}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "GPU",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
